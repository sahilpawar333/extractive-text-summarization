{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk numpy networkx scikit-learn gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hblUtEsdT_qG",
        "outputId": "1acb51bc-303c-4190-edd0-32c022fbb54e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import KMeans\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "nltk.download('maxent_ne_chunker', quiet=True)\n",
        "nltk.download('words', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-wy_eSCULMF",
        "outputId": "ff516d3a-4496-47b1-fb12-5d7290d5434e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedNewsArticleSummarizer:\n",
        "    def __init__(self, compression_ratio=0.4):\n",
        "        \"\"\"\n",
        "        Initialize the summarizer with enhanced capabilities\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        compression_ratio : float\n",
        "            The ratio of sentences to keep in the summary (0.0 to 1.0)\n",
        "        \"\"\"\n",
        "        self.compression_ratio = compression_ratio\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.ps = PorterStemmer()\n",
        "\n",
        "\n",
        "        # Load paraphrasing rules\n",
        "        self.paraphrase_rules = self._load_paraphrase_rules()\n",
        "\n",
        "        # Transition phrases for better coherence\n",
        "        self.transition_phrases = {\n",
        "            'addition': ['Additionally', 'Furthermore', 'Moreover', 'Also'],\n",
        "            'contrast': ['However', 'Nevertheless', 'In contrast', 'On the other hand'],\n",
        "            'cause': ['Therefore', 'As a result', 'Consequently', 'Thus'],\n",
        "            'time': ['Meanwhile', 'Subsequently', 'Later', 'Previously'],\n",
        "            'example': ['For instance', 'For example', 'Specifically'],\n",
        "            'emphasis': ['Notably', 'Importantly', 'Significantly'],\n",
        "            'conclusion': ['In conclusion', 'Ultimately', 'Finally']\n",
        "        }\n",
        "\n",
        "    def _load_paraphrase_rules(self):\n",
        "        \"\"\"Load rules for enhanced rule-based paraphrasing\"\"\"\n",
        "        return {\n",
        "            # Attribution and Reporting\n",
        "            r'according to .+?,': '',\n",
        "            r'as per the .+?,': '',\n",
        "            r'as stated by .+?,': '',\n",
        "            r'in a statement,': '',\n",
        "            r'reportedly': '',\n",
        "            r'it is reported that': '',\n",
        "            r'it has been reported that': '',\n",
        "            r'said that': 'stated',\n",
        "            r'pointed out that': 'indicated',\n",
        "            r'mentioned that': 'noted',\n",
        "            r'emphasized that': 'stressed',\n",
        "            r'highlighted that': 'emphasized',\n",
        "            r'expressed that': 'stated',\n",
        "            r'claimed that': 'stated',\n",
        "            r'confirmed that': 'stated',\n",
        "            r'argued that': 'stated',\n",
        "            r'asserted that': 'stated',\n",
        "            r'added that': 'stated',\n",
        "\n",
        "            # Formal and Redundant Phrases\n",
        "            r'it is important to note that': '',\n",
        "            r'it should be noted that': '',\n",
        "            r'it is worth mentioning that': '',\n",
        "            r'it is to be noted that': '',\n",
        "            r'it must be mentioned that': '',\n",
        "            r'in order to': 'to',\n",
        "            r'in terms of': 'regarding',\n",
        "            r'with regard to': 'regarding',\n",
        "            r'with respect to': 'regarding',\n",
        "            r'in light of the fact that': 'because',\n",
        "            r'due to the fact that': 'because',\n",
        "            r'for the purpose of': 'for',\n",
        "            r'at this point in time': 'now',\n",
        "            r'at the present moment': 'now',\n",
        "            r'in the near future': 'soon',\n",
        "            r'in the coming days': 'soon',\n",
        "            r'for all intents and purposes': '',\n",
        "            r'for the most part': '',\n",
        "            r'as a matter of fact': '',\n",
        "            r'with the aim of': 'to',\n",
        "            r'notwithstanding the fact that': 'despite',\n",
        "\n",
        "            # Weak and Filler Words\n",
        "            r'actually': '',\n",
        "            r'basically': '',\n",
        "            r'currently': '',\n",
        "            r'generally': '',\n",
        "            r'simply': '',\n",
        "            r'that is to say': '',\n",
        "            r'to put it simply': '',\n",
        "            r'needless to say': '',\n",
        "            r'to be honest': '',\n",
        "            r'frankly speaking': '',\n",
        "            r'in fact': '',\n",
        "            r'in other words': '',\n",
        "\n",
        "            # Transition and Linking Phrases\n",
        "            r'on the other hand,': 'however,',\n",
        "            r'in contrast,': 'however,',\n",
        "            r'at the same time,': 'meanwhile,',\n",
        "            r'after all,': '',\n",
        "            r'to that end,': '',\n",
        "            r'by the same token,': '',\n",
        "            r'as a result of': 'due to',\n",
        "            r'due to the fact that': 'because',\n",
        "            r'as far as .*? is concerned,': '',\n",
        "            r'if truth be told,': '',\n",
        "            r'to sum up,': 'in conclusion,',\n",
        "            r'in the final analysis,': 'ultimately,',\n",
        "            r'in the meantime,': 'meanwhile,',\n",
        "            r'whatâ€™s more,': 'moreover,',\n",
        "            r'taking into account the fact that': 'considering',\n",
        "\n",
        "            # Opinion and Value Statements\n",
        "            r'it is believed that': 'experts believe',\n",
        "            r'some people think that': 'some believe',\n",
        "            r'it is widely accepted that': 'it is accepted that',\n",
        "            r'there is no doubt that': 'clearly,',\n",
        "            r'it is true that': '',\n",
        "            r'it is possible that': 'perhaps',\n",
        "            r'it would appear that': 'apparently',\n",
        "            r'it is assumed that': 'it is believed that',\n",
        "\n",
        "            # Common Press and News Jargon\n",
        "            r'breaking news:': '',\n",
        "            r'sources said that': 'sources stated',\n",
        "            r'close sources stated that': 'sources stated',\n",
        "            r'in an exclusive interview,': '',\n",
        "            r'our correspondent reported that': '',\n",
        "            r'officials have confirmed that': 'officials confirmed',\n",
        "            r'recent developments suggest that': 'recent reports suggest',\n",
        "\n",
        "            # Legal and Political Terms\n",
        "            r'legally speaking,': '',\n",
        "            r'politically speaking,': '',\n",
        "            r'it remains to be seen whether': 'whether',\n",
        "            r'to the best of my knowledge,': '',\n",
        "            r'from a legal perspective,': '',\n",
        "            r'from a political perspective,': '',\n",
        "\n",
        "            # Common Numeric/Statistical Phrases\n",
        "            r'approximately': 'about',\n",
        "            r'around': 'about',\n",
        "            r'in excess of': 'more than',\n",
        "            r'not less than': 'at least',\n",
        "            r'less than': 'under',\n",
        "            r'over a period of': 'in',\n",
        "            r'at a rate of': 'at',\n",
        "\n",
        "            # Date and Time Phrases\n",
        "            r'at this time': 'now',\n",
        "            r'at present': 'now',\n",
        "            r'in the not-so-distant future': 'soon',\n",
        "            r'in recent days': 'recently',\n",
        "            r'for the time being': 'currently',\n",
        "\n",
        "            # Miscellaneous\n",
        "            r'on a daily basis': 'daily',\n",
        "            r'on a regular basis': 'regularly',\n",
        "            r'to a certain extent': 'somewhat',\n",
        "            r'to some degree': 'somewhat',\n",
        "            r'from time to time': 'occasionally',\n",
        "            r'in the event that': 'if',\n",
        "            r'under the circumstances': 'considering',\n",
        "            r'with the exception of': 'except for',\n",
        "            r'in spite of the fact that': 'despite',\n",
        "            r'in the vicinity of': 'near',\n",
        "            r'if and when': 'if',\n",
        "            r'in connection with': 'regarding',\n",
        "        }\n",
        "\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Clean and tokenize the text into sentences and words\"\"\"\n",
        "        # Clean text\n",
        "        text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
        "        text = re.sub(r'\\[[0-9]*\\]', '', text)  # Remove citations\n",
        "\n",
        "        # Tokenize into sentences\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        # Process each sentence\n",
        "        clean_sentences = []\n",
        "        tokenized_sentences = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            # Remove punctuation\n",
        "            clean_sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "            clean_sentence = clean_sentence.lower()\n",
        "\n",
        "            # Tokenize the sentence\n",
        "            words = word_tokenize(clean_sentence)\n",
        "\n",
        "            # Remove stopwords and stem\n",
        "            filtered_words = []\n",
        "            for word in words:\n",
        "                if word not in self.stop_words:\n",
        "                    filtered_words.append(self.ps.stem(word))\n",
        "\n",
        "            clean_sentences.append(clean_sentence)\n",
        "            tokenized_sentences.append(filtered_words)\n",
        "\n",
        "        return sentences, clean_sentences, tokenized_sentences\n",
        "\n",
        "    def extract_entities(self, text):\n",
        "        \"\"\"Extract named entities from text\"\"\"\n",
        "        try:\n",
        "            words = word_tokenize(text)\n",
        "            pos_tags = nltk.pos_tag(words)\n",
        "            named_entities = nltk.ne_chunk(pos_tags)\n",
        "\n",
        "            entities = []\n",
        "            for chunk in named_entities:\n",
        "                if hasattr(chunk, 'label'):\n",
        "                    entity = ' '.join(c[0] for c in chunk)\n",
        "                    entities.append(entity)\n",
        "\n",
        "            return entities\n",
        "        except:\n",
        "            # Fallback if NER fails\n",
        "            return []\n",
        "\n",
        "    def extract_time_references(self, text):\n",
        "        \"\"\"Extract time references from text for chronological ordering\"\"\"\n",
        "        time_patterns = [\n",
        "            r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2}(?:st|nd|rd|th)?,\\s+\\d{4}\\b',\n",
        "            r'\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{1,2}(?:st|nd|rd|th)?,\\s+\\d{4}\\b',\n",
        "            r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b',\n",
        "            r'\\b\\d{4}-\\d{2}-\\d{2}\\b',\n",
        "            r'\\byesterday\\b',\n",
        "            r'\\btoday\\b',\n",
        "            r'\\btomorrow\\b',\n",
        "            r'\\blast (?:week|month|year)\\b',\n",
        "            r'\\bnext (?:week|month|year)\\b',\n",
        "            r'\\b(?:Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)\\b',\n",
        "        ]\n",
        "\n",
        "        time_references = []\n",
        "        for pattern in time_patterns:\n",
        "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            time_references.extend(matches)\n",
        "\n",
        "        return time_references\n",
        "\n",
        "    def textrank_score(self, sentences, clean_sentences):\n",
        "        \"\"\"Score sentences using the TextRank algorithm\"\"\"\n",
        "        if len(clean_sentences) < 2:\n",
        "            return [(0, 1.0)] if clean_sentences else []\n",
        "\n",
        "        # Create a similarity matrix\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        tfidf_matrix = vectorizer.fit_transform(clean_sentences)\n",
        "\n",
        "        # Calculate similarity between sentences\n",
        "        similarity_matrix = tfidf_matrix * tfidf_matrix.T\n",
        "\n",
        "        # Create a graph from the similarity matrix\n",
        "        nx_graph = nx.from_scipy_sparse_array(similarity_matrix)\n",
        "\n",
        "        # Apply PageRank algorithm\n",
        "        scores = nx.pagerank(nx_graph)\n",
        "\n",
        "        # Convert scores to a list\n",
        "        ranked_sentences = [(i, scores[i]) for i in range(len(sentences))]\n",
        "\n",
        "        return ranked_sentences\n",
        "\n",
        "    def tfidf_score(self, sentences, clean_sentences):\n",
        "        \"\"\"Score sentences based on TF-IDF weights\"\"\"\n",
        "        if len(clean_sentences) < 2:\n",
        "            return [(0, 1.0)] if clean_sentences else []\n",
        "\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        tfidf_matrix = vectorizer.fit_transform(clean_sentences)\n",
        "\n",
        "        # Get feature names\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "        # Score sentences based on the sum of TF-IDF values\n",
        "        scores = []\n",
        "        for i, sentence in enumerate(clean_sentences):\n",
        "            # Get the TF-IDF values for this sentence\n",
        "            feature_index = tfidf_matrix[i, :].nonzero()[1]\n",
        "            tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])\n",
        "\n",
        "            # Calculate sentence score (sum of TF-IDF values)\n",
        "            score = sum(val for _, val in tfidf_scores)\n",
        "            scores.append((i, score))\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def luhn_score(self, sentences, tokenized_sentences):\n",
        "        \"\"\"Score sentences using Luhn's algorithm based on significant words\"\"\"\n",
        "        if not tokenized_sentences:\n",
        "            return []\n",
        "\n",
        "        # Calculate word frequency\n",
        "        all_words = [word for sentence in tokenized_sentences for word in sentence]\n",
        "        word_freq = Counter(all_words)\n",
        "\n",
        "        # Find significant words (above threshold)\n",
        "        threshold = sum(word_freq.values()) / len(word_freq) if word_freq else 0\n",
        "        significant_words = [word for word, freq in word_freq.items() if freq > threshold]\n",
        "\n",
        "        # Score sentences based on clusters of significant words\n",
        "        scores = []\n",
        "        for i, sentence in enumerate(tokenized_sentences):\n",
        "            word_positions = [j for j, word in enumerate(sentence) if word in significant_words]\n",
        "\n",
        "            if not word_positions:\n",
        "                scores.append((i, 0))\n",
        "                continue\n",
        "\n",
        "            clusters = []\n",
        "            if word_positions:\n",
        "                current_cluster = [word_positions[0]]\n",
        "\n",
        "                for pos in word_positions[1:]:\n",
        "                    if pos - current_cluster[-1] < 3:  # Words less than 3 positions apart form a cluster\n",
        "                        current_cluster.append(pos)\n",
        "                    else:\n",
        "                        clusters.append(current_cluster)\n",
        "                        current_cluster = [pos]\n",
        "\n",
        "                if current_cluster:\n",
        "                    clusters.append(current_cluster)\n",
        "\n",
        "            # Calculate score based on the square of significant words divided by total words\n",
        "            max_cluster_size = max([len(cluster) for cluster in clusters]) if clusters else 0\n",
        "            score = (max_cluster_size ** 2) / len(sentence) if len(sentence) > 0 else 0\n",
        "            scores.append((i, score))\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def lsa_score(self, clean_sentences):\n",
        "        \"\"\"Score sentences using Latent Semantic Analysis\"\"\"\n",
        "        if len(clean_sentences) < 2:\n",
        "            return [(0, 1.0)] if clean_sentences else []\n",
        "\n",
        "        # Create TF-IDF matrix\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        tfidf_matrix = vectorizer.fit_transform(clean_sentences)\n",
        "\n",
        "        # Apply SVD\n",
        "        n_components = min(len(clean_sentences) - 1, 5)  # Number of topics\n",
        "        n_components = max(1, n_components)\n",
        "        svd = TruncatedSVD(n_components=n_components)\n",
        "        svd.fit(tfidf_matrix)\n",
        "\n",
        "        # Get sentence scores based on the first singular vector\n",
        "        u = svd.components_[0]\n",
        "        sentence_scores = [(i, abs(u[i])) for i in range(len(clean_sentences))]\n",
        "\n",
        "        return sentence_scores\n",
        "\n",
        "    def entity_score(self, sentences, tokenized_sentences):\n",
        "        \"\"\"Score sentences based on named entity presence\"\"\"\n",
        "        # Extract all entities from the full text\n",
        "        full_text = ' '.join(sentences)\n",
        "        all_entities = self.extract_entities(full_text)\n",
        "\n",
        "        if not all_entities:\n",
        "            # Fallback to important noun phrases if NER fails\n",
        "            all_words = [word for sentence in tokenized_sentences for word in sentence]\n",
        "            word_freq = Counter(all_words)\n",
        "            important_words = [word for word, freq in word_freq.most_common(10)]\n",
        "\n",
        "            scores = []\n",
        "            for i, sentence in enumerate(tokenized_sentences):\n",
        "                if not sentence:\n",
        "                    scores.append((i, 0))\n",
        "                    continue\n",
        "\n",
        "                # Count important words in sentence\n",
        "                count = sum(1 for word in sentence if word in important_words)\n",
        "                score = count / len(sentence) if sentence else 0\n",
        "                scores.append((i, score))\n",
        "\n",
        "            return scores\n",
        "\n",
        "        # Score sentences based on entity presence\n",
        "        scores = []\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            # Count entities in this sentence\n",
        "            entity_count = sum(1 for entity in all_entities if entity.lower() in sentence.lower())\n",
        "\n",
        "            # Calculate score based on entity density\n",
        "            words = len(tokenized_sentences[i]) if i < len(tokenized_sentences) and tokenized_sentences[i] else 1\n",
        "            score = entity_count / words if words > 0 else 0\n",
        "            scores.append((i, score))\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def numeric_score(self, sentences):\n",
        "        \"\"\"Score sentences based on presence of numbers/statistics\"\"\"\n",
        "        scores = []\n",
        "\n",
        "        # Regular expression for numbers\n",
        "        number_pattern = re.compile(r'\\b\\d+(?:\\.\\d+)?%?\\b')\n",
        "\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            # Count numbers in sentence\n",
        "            numbers = number_pattern.findall(sentence)\n",
        "\n",
        "            # Score based on number presence\n",
        "            score = min(1.0, len(numbers) * 0.25)  # Cap at 1.0\n",
        "            scores.append((i, score))\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def quote_score(self, sentences):\n",
        "        \"\"\"Score sentences containing quotes\"\"\"\n",
        "        scores = []\n",
        "\n",
        "        # Regular expression for quotes\n",
        "        quote_pattern = re.compile(r'[\"\\'](.*?)[\"\\']')\n",
        "\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            # Check if sentence contains a quote\n",
        "            quotes = quote_pattern.findall(sentence)\n",
        "\n",
        "            # Score based on quote presence\n",
        "            score = 1.0 if quotes else 0.0\n",
        "            scores.append((i, score))\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def _calculate_position_score(self, i, n):\n",
        "        \"\"\"Calculate position score based on sentence position\"\"\"\n",
        "        # Position-based scoring (modified for news articles)\n",
        "        if i == 0:\n",
        "            score = 1.0  # First sentence (headline)\n",
        "        elif i == 1:\n",
        "            score = 0.95  # Second sentence (often contains key info)\n",
        "        elif i < n * 0.1:\n",
        "            score = 0.9  # First 10% (lead paragraph)\n",
        "        elif i < n * 0.2:\n",
        "            score = 0.8  # First 20%\n",
        "        elif i < n * 0.3:\n",
        "            score = 0.7  # First 30%\n",
        "        elif i > n * 0.8:\n",
        "            score = 0.6  # Last 20% (conclusion, future implications)\n",
        "        else:\n",
        "            score = 0.5  # Middle sentences\n",
        "\n",
        "        return score\n",
        "\n",
        "    def position_score(self, sentences):\n",
        "        \"\"\"Score sentences based on their position in the document\"\"\"\n",
        "        scores = []\n",
        "        n = len(sentences)\n",
        "\n",
        "        for i in range(n):\n",
        "            scores.append((i, self._calculate_position_score(i, n)))\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def title_similarity_score(self, title, clean_sentences, tokenized_sentences):\n",
        "        \"\"\"Score sentences based on similarity to the title\"\"\"\n",
        "        if not title:\n",
        "            return [(i, 0) for i in range(len(clean_sentences))]\n",
        "\n",
        "        # Clean and tokenize the title\n",
        "        clean_title = title.translate(str.maketrans('', '', string.punctuation)).lower()\n",
        "        title_words = [self.ps.stem(word) for word in word_tokenize(clean_title)\n",
        "                      if word not in self.stop_words]\n",
        "\n",
        "        # Score sentences based on overlap with title words\n",
        "        scores = []\n",
        "        for i, sentence in enumerate(tokenized_sentences):\n",
        "            if not sentence or not title_words:\n",
        "                scores.append((i, 0))\n",
        "                continue\n",
        "\n",
        "            # Count words in both title and sentence\n",
        "            overlap = sum(1 for word in sentence if word in title_words)\n",
        "\n",
        "            # Calculate score as ratio of overlap to possible overlap\n",
        "            score = overlap / len(title_words) if len(title_words) > 0 else 0\n",
        "            scores.append((i, score))\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def cohesion_score(self, sentences, tokenized_sentences):\n",
        "        \"\"\"Score sentences based on cohesion with neighboring sentences\"\"\"\n",
        "        scores = []\n",
        "        n = len(sentences)\n",
        "\n",
        "        for i in range(n):\n",
        "            # Get neighboring sentences\n",
        "            prev_idx = max(0, i - 1)\n",
        "            next_idx = min(n - 1, i + 1)\n",
        "\n",
        "            # Calculate similarity with neighbors\n",
        "            prev_sim = self._sentence_similarity_tokens(tokenized_sentences[i], tokenized_sentences[prev_idx]) if i > 0 else 0\n",
        "            next_sim = self._sentence_similarity_tokens(tokenized_sentences[i], tokenized_sentences[next_idx]) if i < n - 1 else 0\n",
        "\n",
        "            # Average similarity\n",
        "            avg_sim = (prev_sim + next_sim) / 2 if i > 0 and i < n - 1 else (prev_sim + next_sim)\n",
        "            scores.append((i, avg_sim))\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def semantic_cluster_score(self, sentences, clean_sentences):\n",
        "        \"\"\"Score sentences based on semantic clustering\"\"\"\n",
        "        if len(clean_sentences) < 2:\n",
        "            return [(0, 1.0)] if clean_sentences else []\n",
        "\n",
        "        # Create TF-IDF matrix for semantic representation\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        tfidf_matrix = vectorizer.fit_transform(clean_sentences)\n",
        "\n",
        "        # Determine optimal number of clusters (topics)\n",
        "        n_clusters = min(max(2, len(sentences) // 5), 8)  # Between 2 and 8 clusters\n",
        "\n",
        "        # Apply K-means clustering\n",
        "        km = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        km.fit(tfidf_matrix)\n",
        "        clusters = km.labels_\n",
        "\n",
        "        # Find sentences closest to cluster centroids\n",
        "        centroids = km.cluster_centers_\n",
        "        central_sentences = {}\n",
        "\n",
        "        for cluster_id in range(n_clusters):\n",
        "            # Get sentences in this cluster\n",
        "            cluster_sentence_indices = [i for i, label in enumerate(clusters) if label == cluster_id]\n",
        "\n",
        "            if not cluster_sentence_indices:\n",
        "                continue\n",
        "\n",
        "            # Find sentence closest to centroid\n",
        "            closest_idx = None\n",
        "            min_distance = float('inf')\n",
        "\n",
        "            for idx in cluster_sentence_indices:\n",
        "                distance = np.linalg.norm(tfidf_matrix[idx].toarray() - centroids[cluster_id])\n",
        "                if distance < min_distance:\n",
        "                    min_distance = distance\n",
        "                    closest_idx = idx\n",
        "\n",
        "            if closest_idx is not None:\n",
        "                central_sentences[closest_idx] = 1.0\n",
        "\n",
        "        # Convert to standard score format\n",
        "        scores = [(i, central_sentences.get(i, 0.0)) for i in range(len(sentences))]\n",
        "        return scores\n",
        "\n",
        "    def topic_coverage_score(self, sentences, clean_sentences, tokenized_sentences):\n",
        "        \"\"\"Score sentences based on topic coverage using LDA\"\"\"\n",
        "        if len(clean_sentences) < 5:  # Need enough sentences for topic modeling\n",
        "            return [(i, 1.0) for i in range(len(clean_sentences))]\n",
        "\n",
        "        # Create dictionary and corpus for LDA\n",
        "        all_tokenized = [sentence for sentence in tokenized_sentences if sentence]\n",
        "        dictionary = corpora.Dictionary(all_tokenized)\n",
        "        corpus = [dictionary.doc2bow(sentence) for sentence in all_tokenized]\n",
        "\n",
        "        # Determine optimal number of topics\n",
        "        n_topics = min(max(2, len(sentences) // 10), 5)  # Between 2 and 5 topics\n",
        "\n",
        "        # Train LDA model\n",
        "        lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=n_topics, passes=10, random_state=42)\n",
        "\n",
        "        # Get topic distribution for each sentence\n",
        "        sentence_topics = [lda[corpus[i]] for i in range(len(corpus))]\n",
        "\n",
        "        # Find most representative sentence for each topic\n",
        "        topic_representatives = {}\n",
        "        for topic_id in range(n_topics):\n",
        "            max_prob = -1\n",
        "            max_sent_idx = -1\n",
        "\n",
        "            for i, sent_topic in enumerate(sentence_topics):\n",
        "                for t, prob in sent_topic:\n",
        "                    if t == topic_id and prob > max_prob:\n",
        "                        max_prob = prob\n",
        "                        max_sent_idx = i\n",
        "\n",
        "            if max_sent_idx != -1:\n",
        "                # Map from tokenized sentence index to original sentence index\n",
        "                original_idx = tokenized_sentences.index(all_tokenized[max_sent_idx]) if all_tokenized[max_sent_idx] in tokenized_sentences else -1\n",
        "                if original_idx != -1:\n",
        "                    topic_representatives[original_idx] = 1.0\n",
        "\n",
        "        # Convert to standard score format\n",
        "        scores = [(i, topic_representatives.get(i, 0.0)) for i in range(len(sentences))]\n",
        "        return scores\n",
        "\n",
        "    def _sentence_similarity_tokens(self, tokens1, tokens2):\n",
        "        \"\"\"Calculate Jaccard similarity between two token lists\"\"\"\n",
        "        if not tokens1 or not tokens2:\n",
        "            return 0\n",
        "\n",
        "        set1 = set(tokens1)\n",
        "        set2 = set(tokens2)\n",
        "\n",
        "        intersection = len(set1.intersection(set2))\n",
        "        union = len(set1.union(set2))\n",
        "\n",
        "        return intersection / union if union > 0 else 0\n",
        "\n",
        "    def optimized_ensemble_score(self, sentences, clean_sentences, tokenized_sentences, title=None):\n",
        "        \"\"\"Combine multiple scoring methods with optimized weights for news articles\"\"\"\n",
        "        # Get individual scores\n",
        "        scores_dict = {}\n",
        "\n",
        "        scores_dict['textrank'] = dict(self.textrank_score(sentences, clean_sentences))\n",
        "        scores_dict['tfidf'] = dict(self.tfidf_score(sentences, clean_sentences))\n",
        "        scores_dict['luhn'] = dict(self.luhn_score(sentences, tokenized_sentences))\n",
        "        scores_dict['lsa'] = dict(self.lsa_score(clean_sentences))\n",
        "        scores_dict['entity'] = dict(self.entity_score(sentences, tokenized_sentences))\n",
        "        scores_dict['numeric'] = dict(self.numeric_score(sentences))\n",
        "        scores_dict['quote'] = dict(self.quote_score(sentences))\n",
        "        scores_dict['position'] = dict(self.position_score(sentences))\n",
        "        scores_dict['cohesion'] = dict(self.cohesion_score(sentences, tokenized_sentences))\n",
        "        scores_dict['semantic'] = dict(self.semantic_cluster_score(sentences, clean_sentences))\n",
        "        scores_dict['topic'] = dict(self.topic_coverage_score(sentences, clean_sentences, tokenized_sentences))\n",
        "\n",
        "        # Title similarity (if title is provided)\n",
        "        if title:\n",
        "            scores_dict['title'] = dict(self.title_similarity_score(title, clean_sentences, tokenized_sentences))\n",
        "        else:\n",
        "            scores_dict['title'] = {i: 0 for i in range(len(sentences))}\n",
        "\n",
        "        # Normalize scores between 0 and 1\n",
        "        def normalize(scores):\n",
        "            max_score = max(scores.values()) if scores.values() and max(scores.values()) > 0 else 1\n",
        "            return {i: (score / max_score if max_score > 0 else 0) for i, score in scores.items()}\n",
        "\n",
        "        # Normalize all score sets\n",
        "        for key in scores_dict:\n",
        "            scores_dict[key] = normalize(scores_dict[key])\n",
        "\n",
        "        # Weights for different methods (optimized for news articles)\n",
        "        weights = {\n",
        "            'textrank': 0.15,    # TextRank captures overall importance\n",
        "            'tfidf': 0.10,       # TF-IDF captures distinctive content\n",
        "            'luhn': 0.05,        # Luhn captures clusters of important words\n",
        "            'lsa': 0.05,         # LSA captures latent topics\n",
        "            'entity': 0.20,      # Entity captures key people/organizations\n",
        "            'numeric': 0.10,     # Numeric captures important statistics\n",
        "            'quote': 0.10,       # Quote captures important statements\n",
        "            'position': 0.20,    # Position captures structural importance\n",
        "            'cohesion': 0.05,    # Cohesion captures narrative flow\n",
        "            'title': 0.05,       # Title similarity captures central theme\n",
        "            'semantic': 0.05,    # Semantic clustering for diversity\n",
        "            'topic': 0.05        # Topic coverage for completeness\n",
        "        }\n",
        "\n",
        "        # Combine scores with weights\n",
        "        ensemble_scores = []\n",
        "        for i in range(len(sentences)):\n",
        "            score = sum(weights[key] * scores_dict[key].get(i, 0) for key in weights)\n",
        "            ensemble_scores.append((i, score))\n",
        "\n",
        "        return ensemble_scores\n",
        "\n",
        "\n",
        "    def select_balanced_sentences(self, scores, sentences, tokenized_sentences):\n",
        "        \"\"\"Select sentences that balance importance, diversity, and coverage\"\"\"\n",
        "        # Sort by score (highest first)\n",
        "        sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Calculate how many sentences to include\n",
        "        n_sentences = max(2, int(len(sentences) * self.compression_ratio))\n",
        "\n",
        "        # Create variables to track selection\n",
        "        selected_indices = []\n",
        "        selected_sentences = []\n",
        "        covered_words = set()\n",
        "        covered_entities = set()\n",
        "\n",
        "        # Always include the highest scoring sentence\n",
        "        top_idx, _ = sorted_scores[0]\n",
        "        selected_indices.append(top_idx)\n",
        "        selected_sentences.append(sentences[top_idx])\n",
        "        covered_words.update(tokenized_sentences[top_idx])\n",
        "\n",
        "        # Extract entities from the selected sentence\n",
        "        entities = self.extract_entities(sentences[top_idx])\n",
        "        covered_entities.update([e.lower() for e in entities])\n",
        "\n",
        "        # Create a candidate list (excluding the already selected sentence)\n",
        "        candidates = sorted_scores[1:]\n",
        "\n",
        "        # Select remaining sentences\n",
        "        while len(selected_indices) < n_sentences and candidates:\n",
        "            best_idx = None\n",
        "            best_score = -1\n",
        "            best_redundancy = 1  # Lower is better\n",
        "\n",
        "            for idx, base_score in candidates:\n",
        "                current_sentence = sentences[idx].lower()\n",
        "                current_tokens = set(tokenized_sentences[idx])\n",
        "\n",
        "                # Extract entities from this sentence\n",
        "                current_entities = [e.lower() for e in self.extract_entities(sentences[idx])]\n",
        "\n",
        "                # Skip if too similar to already selected sentences\n",
        "                too_similar = False\n",
        "                for selected_sentence in selected_sentences:\n",
        "                    similarity = self._sentence_similarity(current_sentence, selected_sentence.lower())\n",
        "                    if similarity > 0.7:  # Threshold for redundancy\n",
        "                        too_similar = True\n",
        "                        break\n",
        "\n",
        "                if too_similar:\n",
        "                    continue\n",
        "\n",
        "                # Calculate coverage gain (how many new words this adds)\n",
        "                new_words = current_tokens - covered_words\n",
        "                coverage_gain = len(new_words) / len(current_tokens) if current_tokens else 0\n",
        "\n",
        "                # Calculate entity coverage gain\n",
        "                new_entities = [e for e in current_entities if e not in covered_entities]\n",
        "                entity_gain = len(new_entities) / max(1, len(current_entities))\n",
        "\n",
        "                # Calculate redundancy\n",
        "                redundancy = 1 - ((coverage_gain + entity_gain) / 2)\n",
        "\n",
        "                # Calculate combined score with multiple factors\n",
        "                combined_score = (base_score * 0.6) + (coverage_gain * 0.2) + (entity_gain * 0.2)\n",
        "\n",
        "                # Select if better than current best\n",
        "                if combined_score > best_score or (combined_score == best_score and redundancy < best_redundancy):\n",
        "                    best_score = combined_score\n",
        "                    best_redundancy = redundancy\n",
        "                    best_idx = idx\n",
        "\n",
        "            if best_idx is not None:\n",
        "                # Add best sentence to selection\n",
        "                selected_indices.append(best_idx)\n",
        "                selected_sentences.append(sentences[best_idx])\n",
        "                covered_words.update(tokenized_sentences[best_idx])\n",
        "                covered_entities.update([e.lower() for e in self.extract_entities(sentences[best_idx])])\n",
        "\n",
        "                # Remove selected sentence from candidates\n",
        "                candidates = [(i, s) for i, s in candidates if i != best_idx]\n",
        "            else:\n",
        "                # If no suitable sentence is found, exit loop\n",
        "                break\n",
        "\n",
        "        # Sort selected indices by their position in the original document\n",
        "        selected_indices.sort()\n",
        "\n",
        "        return selected_indices\n",
        "\n",
        "    def _sentence_similarity(self, sentence1, sentence2):\n",
        "        \"\"\"Calculate cosine similarity between two sentences\"\"\"\n",
        "        # Tokenize\n",
        "        words1 = sentence1.split()\n",
        "        words2 = sentence2.split()\n",
        "\n",
        "        # Create word sets\n",
        "        set1 = set(words1)\n",
        "        set2 = set(words2)\n",
        "\n",
        "        # Calculate Jaccard similarity\n",
        "        intersection = len(set1.intersection(set2))\n",
        "        union = len(set1.union(set2))\n",
        "\n",
        "        return intersection / union if union > 0 else 0\n",
        "\n",
        "    def _get_transition_phrase(self, current_idx, prev_idx, sentences):\n",
        "        \"\"\"Determine appropriate transition phrase based on relationship between sentences\"\"\"\n",
        "        # If first sentence, no transition needed\n",
        "        if prev_idx is None:\n",
        "            return \"\"\n",
        "\n",
        "        current_sentence = sentences[current_idx]\n",
        "        prev_sentence = sentences[prev_idx]\n",
        "\n",
        "        # Look for time references\n",
        "        current_time_refs = self.extract_time_references(current_sentence)\n",
        "        prev_time_refs = self.extract_time_references(prev_sentence)\n",
        "\n",
        "        # Check for contrast indicators\n",
        "        contrast_words = [\"but\", \"however\", \"although\", \"though\", \"yet\", \"despite\", \"nevertheless\", \"on the other hand\", \"in contrast\"]\n",
        "        has_contrast = any(word in current_sentence.lower() for word in contrast_words)\n",
        "\n",
        "        # Check for cause-effect indicators\n",
        "        cause_words = [\"because\", \"since\", \"as a result\", \"therefore\", \"thus\", \"consequently\", \"hence\"]\n",
        "        has_cause = any(word in current_sentence.lower() for word in cause_words)\n",
        "\n",
        "        # Check for additional information\n",
        "        addition_words = [\"also\", \"additionally\", \"furthermore\", \"moreover\", \"in addition\", \"besides\"]\n",
        "        has_addition = any(word in current_sentence.lower() for word in addition_words)\n",
        "\n",
        "        # Check for examples\n",
        "        example_words = [\"for example\", \"for instance\", \"such as\", \"like\"]\n",
        "        has_example = any(word in current_sentence.lower() for word in example_words)\n",
        "\n",
        "        # Determine appropriate transition\n",
        "        if has_contrast:\n",
        "            return self.transition_phrases[\"contrast\"][0] + \", \"\n",
        "        elif has_cause:\n",
        "            return self.transition_phrases[\"cause\"][0] + \", \"\n",
        "        elif has_addition:\n",
        "            return self.transition_phrases[\"addition\"][0] + \", \"\n",
        "        elif has_example:\n",
        "            return self.transition_phrases[\"example\"][0] + \", \"\n",
        "        elif current_time_refs and prev_time_refs:\n",
        "            return self.transition_phrases[\"time\"][0] + \", \"\n",
        "        else:\n",
        "            # No specific relationship detected\n",
        "            return \"\"\n",
        "\n",
        "    def _simple_paraphrase(self, sentence):\n",
        "        \"\"\"Apply rule-based paraphrasing to simplify sentences\"\"\"\n",
        "        paraphrased = sentence\n",
        "\n",
        "        for pattern, replacement in self.paraphrase_rules.items():\n",
        "            paraphrased = re.sub(pattern, replacement, paraphrased)\n",
        "\n",
        "        return paraphrased\n",
        "\n",
        "    def _enhance_coherence(self, selected_sentences, original_indices, sentences):\n",
        "        \"\"\"Enhance coherence by adding transition phrases and light paraphrasing\"\"\"\n",
        "        coherent_summary = []\n",
        "        prev_idx = None\n",
        "\n",
        "        for i, sentence_idx in enumerate(original_indices):\n",
        "            sentence = sentences[sentence_idx]\n",
        "\n",
        "            # Simplify sentence through light paraphrasing\n",
        "            simplified = self._simple_paraphrase(sentence)\n",
        "\n",
        "            # Add appropriate transition phrase\n",
        "            transition = self._get_transition_phrase(sentence_idx, prev_idx, sentences)\n",
        "\n",
        "            # For first sentence, no transition needed\n",
        "            if i == 0:\n",
        "                coherent_summary.append(simplified)\n",
        "            else:\n",
        "                # Add transition phrase or use the original sentence\n",
        "                if transition and not any(simplified.startswith(t) for t in sum(self.transition_phrases.values(), [])):\n",
        "                    coherent_summary.append(transition + simplified[0].lower() + simplified[1:])\n",
        "                else:\n",
        "                    coherent_summary.append(simplified)\n",
        "\n",
        "            prev_idx = sentence_idx\n",
        "\n",
        "        return coherent_summary\n",
        "\n",
        "    def summarize(self, text, title=None):\n",
        "        \"\"\"\n",
        "        Generate a summary of the given text\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        text : str\n",
        "            The text to summarize\n",
        "        title : str, optional\n",
        "            The title of the text\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        str\n",
        "            The generated summary\n",
        "        \"\"\"\n",
        "        # Preprocess text\n",
        "        sentences, clean_sentences, tokenized_sentences = self.preprocess_text(text)\n",
        "\n",
        "        if not sentences:\n",
        "            return \"\"\n",
        "\n",
        "        # Score sentences\n",
        "        scores = self.optimized_ensemble_score(sentences, clean_sentences, tokenized_sentences, title)\n",
        "\n",
        "        # Select sentences\n",
        "        selected_indices = self.select_balanced_sentences(scores, sentences, tokenized_sentences)\n",
        "\n",
        "        # Enhance coherence\n",
        "        coherent_summary = self._enhance_coherence(\n",
        "            [sentences[idx] for idx in selected_indices],\n",
        "            selected_indices,\n",
        "            sentences\n",
        "        )\n",
        "\n",
        "        # Join sentences into summary\n",
        "        summary = \" \".join(coherent_summary)\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def evaluate_summary(self, original_text, reference_summary, generated_summary):\n",
        "        \"\"\"Evaluate summary without external ROUGE packages\"\"\"\n",
        "\n",
        "        def get_ngrams(text, n):\n",
        "            \"\"\"Get n-grams from text\"\"\"\n",
        "            words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "            ngrams = [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n",
        "            return set(ngrams)\n",
        "\n",
        "        # Get words and n-grams\n",
        "        ref_words = set(re.findall(r'\\b\\w+\\b', reference_summary.lower()))\n",
        "        gen_words = set(re.findall(r'\\b\\w+\\b', generated_summary.lower()))\n",
        "\n",
        "        ref_bigrams = get_ngrams(reference_summary, 2)\n",
        "        gen_bigrams = get_ngrams(generated_summary, 2)\n",
        "\n",
        "        # Calculate basic overlap metrics\n",
        "        word_overlap = len(ref_words.intersection(gen_words))\n",
        "        bigram_overlap = len(ref_bigrams.intersection(gen_bigrams))\n",
        "\n",
        "        # Calculate precision, recall, and F1\n",
        "        rouge_1 = 2 * word_overlap / (len(ref_words) + len(gen_words)) if (len(ref_words) + len(gen_words)) > 0 else 0\n",
        "        rouge_2 = 2 * bigram_overlap / (len(ref_bigrams) + len(gen_bigrams)) if (len(ref_bigrams) + len(gen_bigrams)) > 0 else 0\n",
        "        rouge_l = (rouge_1 + rouge_2) / 2  # Simple approximation\n",
        "\n",
        "        # Compression ratio\n",
        "        original_words = len(re.findall(r'\\b\\w+\\b', original_text))\n",
        "        summary_words = len(re.findall(r'\\b\\w+\\b', generated_summary))\n",
        "        compression_ratio = summary_words / original_words if original_words > 0 else 0\n",
        "\n",
        "        # Return metrics\n",
        "        metrics = {\n",
        "            'rouge-1': rouge_1,\n",
        "            'rouge-2': rouge_2,\n",
        "            'rouge-l': rouge_l,\n",
        "            'compression_ratio': compression_ratio,\n",
        "            'summary_word_count': summary_words,\n",
        "            'original_word_count': original_words\n",
        "        }\n",
        "\n",
        "        return metrics"
      ],
      "metadata": {
        "id": "uJPKh1INUYj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Sample news article\n",
        "    article = \"\"\"\n",
        "    Karnataka Deputy Chief Minister DK Shivakumar on Saturday urged women to gear up for the upcoming Assembly and Lok Sabha elections, emphasizing the impact of the Women's Reservation Bill.\n",
        "According to news agency ANI, he was speaking at an International Women's Day event, he said, \"The Women's Reservation Bill is expected to take effect in 2028. Prepare to contest the electionsâ€”no one can stop you in a democratic system.\"\n",
        "Highlighting the growing role of women in governance, he noted that women hold power from the panchayat level to Parliament. \"While there is already 50% reservation for women in Panchayats, many men still control decision-making through female family members. This will change as women become fully capable of leading on their own,\" he added.\n",
        "He noted that the previous UPA government at the Centre had tried to pass the Women's Reservation Bill but could not do so due to \"certain reasons.\"\n",
        "\"The Congress government under the leadership of Sonia Gandhi and Manmohan Singh was planning to bring a women's reservation bill but could not due to certain reasons. The Bill has been passed, and it will take effect for the upcoming Assembly and Lok Sabha elections. We are not sure who will have to lose the seats because of this new reservation,\" he said.\n",
        "He said that women play an important role in their families and society.\n",
        "\"There are umpteen examples in history which highlight the importance of women in our society. Basavanna rightly called them punya sthree,\" he said.\n",
        "\"Women are making waves in all fields. They have the ability to rule the country in the future. Indira Gandhi had already set a precedent. Women's reservation bill changes a lot of things in the days to come,\" he added.\n",
        "Shivakumar said that four of five guarantees of the Congress government in the State were \"directly empowering women.\"\n",
        "\"Women's day events must be fully organised and managed by women. Male officials should not be associated with such events in the future. I have come to this region to take the blessings of women of Kalyana Karnataka, though there was a major event in Bengaluru,\" he said.\n",
        "\"Four of five guarantees are directly empowering women. Our government is committed to empowering women economically,\" he added.\"\"\"\n",
        "\n",
        "    # Example of how to train with human summaries (if available)\n",
        "    human_summary = \"\"\"Karnataka Deputy CM DK Shivakumar, speaking at an International Women's Day event, urged women to prepare for the upcoming Assembly and Lok Sabha elections, highlighting the impact of the Women's Reservation Bill, which is expected to take effect in 2028. He emphasized the growing role of women in governance and noted that while there is already 50% reservation for women in Panchayats, decision-making is still influenced by men. He acknowledged the previous UPA government's attempt to pass the bill and praised women's increasing influence in society. He also mentioned that four of five Congress government guarantees in the state empower women directly. \"\"\"\n",
        "\n",
        "\n",
        "    # Create summarizer with ML enabled and training data\n",
        "    summarizer = EnhancedNewsArticleSummarizer(compression_ratio=0.3)\n",
        "\n",
        "    # Generate summary\n",
        "    title = \"Karnataka Women Empowerment\"\n",
        "    summary = summarizer.summarize(article, title=title)\n",
        "\n",
        "    print(\"Original Article:\\n\", article)\n",
        "    print(\"\\nGenerated Summary:\\n\", summary)\n",
        "\n",
        "\n",
        "    # Evaluate summary quality\n",
        "    metrics = summarizer.evaluate_summary(article, human_summary, summary)\n",
        "    print(\"\\nEvaluation Metrics:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJyn5iIxUlYw",
        "outputId": "fc279e9f-9d8b-4d07-ca74-0ff176a2341a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Article:\n",
            " \n",
            "    Karnataka Deputy Chief Minister DK Shivakumar on Saturday urged women to gear up for the upcoming Assembly and Lok Sabha elections, emphasizing the impact of the Women's Reservation Bill.\n",
            "According to news agency ANI, he was speaking at an International Women's Day event, he said, \"The Women's Reservation Bill is expected to take effect in 2028. Prepare to contest the electionsâ€”no one can stop you in a democratic system.\"\n",
            "Highlighting the growing role of women in governance, he noted that women hold power from the panchayat level to Parliament. \"While there is already 50% reservation for women in Panchayats, many men still control decision-making through female family members. This will change as women become fully capable of leading on their own,\" he added.\n",
            "He noted that the previous UPA government at the Centre had tried to pass the Women's Reservation Bill but could not do so due to \"certain reasons.\"\n",
            "\"The Congress government under the leadership of Sonia Gandhi and Manmohan Singh was planning to bring a women's reservation bill but could not due to certain reasons. The Bill has been passed, and it will take effect for the upcoming Assembly and Lok Sabha elections. We are not sure who will have to lose the seats because of this new reservation,\" he said.\n",
            "He said that women play an important role in their families and society.\n",
            "\"There are umpteen examples in history which highlight the importance of women in our society. Basavanna rightly called them punya sthree,\" he said.\n",
            "\"Women are making waves in all fields. They have the ability to rule the country in the future. Indira Gandhi had already set a precedent. Women's reservation bill changes a lot of things in the days to come,\" he added.\n",
            "Shivakumar said that four of five guarantees of the Congress government in the State were \"directly empowering women.\"\n",
            "\"Women's day events must be fully organised and managed by women. Male officials should not be associated with such events in the future. I have come to this region to take the blessings of women of Kalyana Karnataka, though there was a major event in Bengaluru,\" he said.\n",
            "\"Four of five guarantees are directly empowering women. Our government is committed to empowering women economically,\" he added.\n",
            "\n",
            "Generated Summary:\n",
            "  Karnataka Deputy Chief Minister DK Shivakumar on Saturday urged women to gear up for the upcoming Assembly and Lok Sabha elections, emphasizing the impact of the Women's Reservation Bill. According to news agency ANI, he was speaking at an International Women's Day event, he said, \"The Women's Reservation Bill is expected to take effect in 2028. \"While there is already 50% reservation for women in Panchayats, many men still control decision-making through female family members. However, he noted that the previous UPA government at the Centre had tried to pass the Women's Reservation Bill but could not do so due to \"certain reasons.\" Women's reservation bill changes a lot of things in the days to come,\" he added. Shivakumar stated four of five guarantees of the Congress government in the State were \"directly empowering women.\"\n",
            "\n",
            "Evaluation Metrics:\n",
            "rouge-1: 0.6428571428571429\n",
            "rouge-2: 0.44541484716157204\n",
            "rouge-l: 0.5441359950093575\n",
            "compression_ratio: 0.36910994764397903\n",
            "summary_word_count: 141\n",
            "original_word_count: 382\n"
          ]
        }
      ]
    }
  ]
}